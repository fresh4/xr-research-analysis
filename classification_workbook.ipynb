{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Imports\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1b7gmvcW_f1"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import umap\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, minmax_scale\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import DBSCAN\n",
        "from scipy.spatial.distance import directed_hausdorff\n",
        "from scipy.spatial.transform import Rotation as R\n",
        "\n",
        "warnings.simplefilter(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Global Variable Definitions\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The \"root\" folder that holds all the experiment data; must follow a strict folder structure.\n",
        "ROOT = \"data\"\n",
        "base_path = os.path.join(os.getcwd(), ROOT)\n",
        "\n",
        "# The experiment type folder to look into, if each experiment has a \"hand\" and a \"controller\" folder.\n",
        "FOLDER = \"hand\"\n",
        "\n",
        "# Number of participants and experiments to index; TODO: Detect this automatically.\n",
        "PARTICIPANTS = 4\n",
        "EXPERIMENTS = 10\n",
        "\n",
        "# The list of files to analyze.\n",
        "FILE_LIST = [\n",
        "        'Ball_1.csv', 'Ball2_1.csv', 'Ball3_1.csv', 'Ball4_1.csv', 'Ball5_1.csv',\n",
        "        'Ball6_1.csv', 'Ball7_1.csv', 'Ball8_1.csv', 'Ball9_1.csv', 'Ball10_1.csv',\n",
        "        'CubeMetal_1.csv', 'CubeNormal_1.csv', 'CubeWood_1.csv',\n",
        "        'Cup_1.csv', 'Cup2_1.csv', 'Hammer_1.csv', 'Mallet_1.csv'\n",
        "    ]\n",
        "\n",
        "# The test size for the test/training data ratio. Standard is 80% training data, 20% test data.\n",
        "TEST_SIZE = 0.6\n",
        "\n",
        "IS_NORMALIZE = True\n",
        "\n",
        "# Various features arrays categorized by the data they hold. \n",
        "imu_cols = [\n",
        "    \"imu_pos_x\",\n",
        "    \"imu_pos_y\", \n",
        "    \"imu_pos_z\", \n",
        "    \"imu_rot_x\", \n",
        "    \"imu_rot_y\", \n",
        "    \"imu_rot_z\"\n",
        "    ]\n",
        "object_cols = [\n",
        "    # \"lv_x\", \"lv_y\", \"lv_z\", \"av_x\", \"av_y\", \"av_z\", \n",
        "    \"pos_x\", \"pos_y\", \"pos_z\"\n",
        "    ]\n",
        "hands_cols = [\n",
        "    # 'lh_pos_x', \"lh_pos_y\", \"lh_pos_z\", \"lh_rot_x\", \"lh_rot_y\", \"lh_rot_z\",\n",
        "    \"rh_pos_x\", \"rh_pos_y\", \"rh_pos_z\", \"rh_rot_x\", \"rh_rot_y\", \"rh_rot_z\"\n",
        "]\n",
        "fingers_cols = [\n",
        "    # \"lh_thumb_x\", \"lh_thumb_y\", \"lh_thumb_z\",\n",
        "    # \"lh_index_x\", \"lh_index_y\", \"lh_index_z\",\n",
        "    # \"lh_middle_x\", \"lh_middle_y\", \"lh_middle_z\",\n",
        "    # \"lh_ring_x\", \"lh_ring_y\", \"lh_ring_z\",\n",
        "    # \"lh_little_x\", \"lh_little_y\", \"lh_little_z\",\n",
        "    \"rh_thumb_x\", \"rh_thumb_y\", \"rh_thumb_z\",\n",
        "    \"rh_index_x\", \"rh_index_y\", \"rh_index_z\",\n",
        "    # \"rh_middle_x\", \"rh_middle_y\", \"rh_middle_z\",\n",
        "    # \"rh_ring_x\", \"rh_ring_y\", \"rh_ring_z\",\n",
        "    \"rh_little_x\", \"rh_little_y\", \"rh_little_z\"\n",
        "]\n",
        "\n",
        "# This is the important variable that will be used by the training model.\n",
        "# Put any physical features you want to include here.\n",
        "FEATURES_COLS = [object_cols, hands_cols, fingers_cols, imu_cols]\n",
        "STAT_FEAT_LEN = 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Extraction\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_features(df: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    This method extracts specific user defined features by processing raw data.\\n\n",
        "    The first section extracts statistical analysis of all columns indiscriminately.\\n\n",
        "    The rest is manual extraction of individual features from the raw data. \n",
        "    \"\"\"\n",
        "    features = []\n",
        "\n",
        "    # Extract statistical features from all RAW DATA columns\n",
        "    for col_group in FEATURES_COLS:\n",
        "        group_data = df[col_group]\n",
        "        # Calculated statistical features, modify as needed\n",
        "        stats = [\n",
        "            group_data.mean(),\n",
        "            group_data.std(),\n",
        "            group_data.max(),\n",
        "            group_data.min(),\n",
        "            group_data.quantile(0.25),\n",
        "            group_data.quantile(0.75),\n",
        "            group_data.skew(),\n",
        "            group_data.kurtosis(),\n",
        "        ]\n",
        "        features.extend( [stat.values for stat in stats] )\n",
        "        # features.extend( [extract_temporal_features(group_data)] )\n",
        "    \n",
        "    # Extract custom processed features.\n",
        "    # Distance between the headset and the right hand.\n",
        "    imu_pos_matrix = np.column_stack( [df[\"imu_pos_x\"], df[\"imu_pos_y\"], df[\"imu_pos_z\"]] )\n",
        "    rh_pos_matrix = np.column_stack( [df[\"rh_pos_x\"], df[\"rh_pos_y\"], df[\"rh_pos_z\"]] )\n",
        "    df[\"hand_head_distance\"] =  np.linalg.norm( np.subtract( imu_pos_matrix, rh_pos_matrix) , axis=1)\n",
        "\n",
        "    # Distance between index and thumb (right hand)\n",
        "    rh_index_matrix = np.column_stack( [ df[\"rh_index_x\"], df[\"rh_index_y\"], df[\"rh_index_z\"] ] )\n",
        "    rh_thumb_matrix = np.column_stack( [ df[\"rh_thumb_x\"], df[\"rh_thumb_y\"], df[\"rh_thumb_z\"] ] )\n",
        "    df[\"thumb_index_distance\"] =  np.linalg.norm( np.subtract(rh_index_matrix, rh_thumb_matrix) , axis=1)\n",
        "\n",
        "    # Distance between index and middle (right hand)\n",
        "    rh_middle_matrix = np.column_stack( [ df[\"rh_middle_x\"], df[\"rh_middle_y\"], df[\"rh_middle_z\"] ] )\n",
        "    df[\"middle_index_distance\"] =  np.linalg.norm( np.subtract(rh_index_matrix, rh_middle_matrix) , axis=1)\n",
        "\n",
        "    # Distance between ring and middle (right hand)\n",
        "    rh_ring_matrix = np.column_stack( [ df[\"rh_ring_x\"], df[\"rh_ring_y\"], df[\"rh_ring_z\"] ] )\n",
        "    df[\"middle_ring_distance\"] =  np.linalg.norm( np.subtract(rh_ring_matrix, rh_middle_matrix) , axis=1)\n",
        "\n",
        "    # Distance between ring and little (right hand)\n",
        "    rh_little_matrix = np.column_stack( [ df[\"rh_little_x\"], df[\"rh_little_y\"], df[\"rh_little_z\"] ] )\n",
        "    df[\"little_ring_distance\"] =  np.linalg.norm( np.subtract(rh_ring_matrix, rh_little_matrix) , axis=1)\n",
        "\n",
        "    # Distances between finger tips and hand\n",
        "    df[\"rh_index_distance\"] =  np.linalg.norm( np.subtract(rh_pos_matrix, rh_index_matrix) , axis=1)\n",
        "    df[\"rh_thumb_distance\"] =  np.linalg.norm( np.subtract(rh_pos_matrix, rh_thumb_matrix) , axis=1)\n",
        "    df[\"rh_middle_distance\"] =  np.linalg.norm( np.subtract(rh_pos_matrix, rh_middle_matrix) , axis=1)\n",
        "    df[\"rh_ring_distance\"] =  np.linalg.norm( np.subtract(rh_pos_matrix, rh_ring_matrix) , axis=1)\n",
        "    df[\"rh_little_distance\"] =  np.linalg.norm( np.subtract(rh_pos_matrix, rh_little_matrix) , axis=1)\n",
        "\n",
        "    # Distance between pinky tip and object\n",
        "    obj_pos_matrix = np.column_stack( [df[\"pos_x\"], df[\"pos_y\"], df[\"pos_z\"]] )\n",
        "    df[\"obj_pinky_distance\"] =  np.linalg.norm( np.subtract( obj_pos_matrix, rh_little_matrix) , axis=1)\n",
        "    \n",
        "    complex_features = [\n",
        "        # df[\"hand_head_distance\"],\n",
        "        # df[\"thumb_index_distance\"],\n",
        "        # df[\"middle_index_distance\"],\n",
        "        # df[\"middle_ring_distance\"],\n",
        "        # df[\"little_ring_distance\"],\n",
        "        # df[\"rh_index_distance\"],\n",
        "        # df[\"rh_middle_distance\"],\n",
        "        # df[\"rh_ring_distance\"],\n",
        "        # df[\"rh_little_distance\"],\n",
        "        # df[\"rh_thumb_distance\"],\n",
        "    ]\n",
        "\n",
        "    for feature in complex_features:\n",
        "        features.extend( [df_to_stats(feature)] )\n",
        "    \n",
        "    return np.concatenate(features)\n",
        "\n",
        "def calculate_feature_importance(rfc: RandomForestClassifier | GradientBoostingClassifier):\n",
        "    # Returns the feature importance list SPECIFICALLY for statistical features\n",
        "    # Since each feature has ~8 statistical features, we sum them up and count them as one\n",
        "    features = [item for sublist in FEATURES_COLS for item in sublist]\n",
        "    feature_importance = pd.Series(rfc.feature_importances_)\n",
        "\n",
        "    idx = 0\n",
        "    result = {}\n",
        "\n",
        "    for feature in features:\n",
        "        total = sum(feature_importance[idx:idx+STAT_FEAT_LEN - 1])\n",
        "        result[feature] = total\n",
        "        idx += STAT_FEAT_LEN\n",
        "    \n",
        "    result = pd.Series(result)\n",
        "    return result\n",
        "\n",
        "def extract_temporal_features(df):\n",
        "    features = []\n",
        "    for col in df.columns:\n",
        "        values = df[col].values\n",
        "\n",
        "        # First-order difference (velocity changes)\n",
        "        diff1 = np.diff(values, n=1)\n",
        "        # Second-order difference (acceleration)\n",
        "        diff2 = np.diff(values, n=2)\n",
        "        # Second-order difference (jerk)\n",
        "        diff3 = np.diff(values, n=3)\n",
        "\n",
        "        # Autocorrelation (lag=1)\n",
        "        autocorr = np.corrcoef(values[:-1], values[1:])[0, 1] if len(values) > 1 else 0\n",
        "\n",
        "        # Store new features\n",
        "        features.extend([\n",
        "            np.mean(diff1), np.std(diff1), np.max(diff1), np.min(diff1),\n",
        "            np.mean(diff2), np.std(diff2), np.max(diff2), np.min(diff2),\n",
        "            np.mean(diff3), np.std(diff3), np.max(diff3), np.min(diff3),\n",
        "            autocorr\n",
        "        ])\n",
        "    return np.array(features)\n",
        "\n",
        "def extract_projection_features(df: pd.DataFrame):\n",
        "    data = df\n",
        "    # Step 1: Compute Relative Position\n",
        "    relative_pos = data[['pos_x', 'pos_y', 'pos_z']].values - data[['imu_pos_x', 'imu_pos_y', 'imu_pos_z']].values\n",
        "\n",
        "    def imu_to_rotation_matrix(imu_rot_x, imu_rot_y, imu_rot_z):\n",
        "        return R.from_euler('xyz', [imu_rot_x, imu_rot_y, imu_rot_z], degrees=False).as_matrix()\n",
        "\n",
        "    def compute_gaze_direction(rotation_matrix):\n",
        "        return np.dot(rotation_matrix, np.array([1, 0, 0]))\n",
        "\n",
        "    def project_trail(imu_pos, rotation_matrix, relative_pos):\n",
        "        gaze_dir = compute_gaze_direction(rotation_matrix)\n",
        "        proj_length = np.dot(relative_pos, gaze_dir)\n",
        "        return imu_pos + gaze_dir * proj_length\n",
        "\n",
        "    # Compute projected trail\n",
        "    projected_trail = np.array([\n",
        "        project_trail(data[['imu_pos_x', 'imu_pos_y', 'imu_pos_z']].iloc[i].values,\n",
        "                      imu_to_rotation_matrix(*data[['imu_rot_x', 'imu_rot_y', 'imu_rot_z']].iloc[i].values),\n",
        "                      relative_pos[i])\n",
        "        for i in range(len(data))\n",
        "    ])\n",
        "\n",
        "    data[['trail_pos_x', 'trail_pos_y', 'trail_pos_z']] = projected_trail\n",
        "\n",
        "    # Step 2: Compare Trajectory Shapes (Centroid & Curvature) \n",
        "    # dispersion is measured as the Euclidean distance between each point and the centroid (mean position) of the trajectory\n",
        "    imu_centroid = data[['imu_pos_x', 'imu_pos_y', 'imu_pos_z']].mean().values\n",
        "    obj_centroid = data[['pos_x', 'pos_y', 'pos_z']].mean().values\n",
        "    trail_centroid = projected_trail.mean(axis=0)\n",
        "\n",
        "    imu_disp = np.linalg.norm(data[['imu_pos_x', 'imu_pos_y', 'imu_pos_z']].values - imu_centroid, axis=1)\n",
        "    obj_disp = np.linalg.norm(data[['pos_x', 'pos_y', 'pos_z']].values - obj_centroid, axis=1)\n",
        "    trail_disp = np.linalg.norm(projected_trail - trail_centroid, axis=1)\n",
        "\n",
        "    # Step 3: Distance Metrics (Hausdorff, measure of similarity between two sets of points) \n",
        "    hausdorff_obj_trail = directed_hausdorff(data[['pos_x', 'pos_y', 'pos_z']].values, projected_trail)[0]\n",
        "\n",
        "    # Step 4: Statistical Analysis\n",
        "    mean_diff_x, mean_diff_y, mean_diff_z = np.mean(np.abs(data[['pos_x', 'pos_y', 'pos_z']].values - projected_trail), axis=0)\n",
        "    std_diff_x, std_diff_y, std_diff_z = np.std(data[['pos_x', 'pos_y', 'pos_z']].values - projected_trail, axis=0)\n",
        "    corr_coeff = np.corrcoef(data[['pos_x', 'pos_y', 'pos_z']].values.flatten(), projected_trail.flatten())[0, 1]\n",
        "\n",
        "    # Prepare data to be written to output\n",
        "    feature_names = [\n",
        "        \"hausdorff_distance_object_vs_projected_trail\", \n",
        "        \"mean_diff_x\", \"mean_diff_y\", \"mean_diff_z\", \n",
        "        \"std_diff_x\", \"std_diff_y\", \"std_diff_z\",\n",
        "        \"correlation_coefficient\",\n",
        "        \"mean_imu_dispersion\", \"std_imu_dispersion\",\n",
        "        \"mean_object_dispersion\", \"std_object_dispersion\",\n",
        "        \"mean_projected_trail_dispersion\", \"std_projected_trail_dispersion\"\n",
        "    ]\n",
        "    \n",
        "    feature_values = [\n",
        "        hausdorff_obj_trail, \n",
        "        mean_diff_x, mean_diff_y, mean_diff_z,\n",
        "        std_diff_x, std_diff_y, std_diff_z,\n",
        "        corr_coeff, \n",
        "        np.mean(imu_disp), np.std(imu_disp),\n",
        "        np.mean(obj_disp), np.std(obj_disp),\n",
        "        np.mean(trail_disp), np.std(trail_disp)\n",
        "    ]\n",
        "\n",
        "    features_df = pd.DataFrame()\n",
        "    for idx, name in enumerate(feature_names):\n",
        "        features_df[name] = feature_values[idx]\n",
        "    \n",
        "    print(features_df)\n",
        "    return features_df\n",
        "\n",
        "def df_to_stats(df: pd.DataFrame):\n",
        "    return np.array([\n",
        "        df.mean(),\n",
        "        df.std(),\n",
        "        df.max(),\n",
        "        df.min(),\n",
        "        df.quantile(0.25),\n",
        "        df.quantile(0.75),\n",
        "        df.skew(),\n",
        "        df.kurtosis(),\n",
        "    ])\n",
        "\n",
        "def load_data(base_path):\n",
        "    data = []\n",
        "    labels = []\n",
        "    successful_files = 0\n",
        "    failed_files = 0\n",
        "    missing_files = 0\n",
        "    normalization_factors = {}\n",
        "\n",
        "    for person in range(1, PARTICIPANTS+1):\n",
        "        for folder in range(1, EXPERIMENTS+1):\n",
        "            for file_name in FILE_LIST:\n",
        "                file_path = os.path.join(base_path, str(person), FOLDER, str(folder), file_name)\n",
        "                if os.path.exists(file_path):\n",
        "                    df = read_csv_file(file_path)\n",
        "                    if IS_NORMALIZE:\n",
        "                        df, normalization_factors = normalize_features(df, \"imu_pos_y\", [\"imu_rot_x\"], user_id=str(person), normalization_factors=normalization_factors)\n",
        "                    if df is not None and not df.empty:\n",
        "                        try:\n",
        "                            features = extract_features(df)\n",
        "                            # features = extract_projection_features(df)\n",
        "                            data.append(features)\n",
        "                            labels.append(person - 1)\n",
        "                            successful_files += 1\n",
        "                            #print(f\"Successfully processed: {file_path}\")\n",
        "                        except Exception as e:\n",
        "                            failed_files += 1\n",
        "                            print(f\"Error processing {file_path}: {str(e)}\")\n",
        "                            print(e.with_traceback())\n",
        "                else:\n",
        "                    missing_files += 1\n",
        "                    print(f\"File does not exist: {file_path}\")\n",
        "\n",
        "    print(f\"\\nProcessing Summary:\")\n",
        "    print(f\"Successful files: {successful_files}\")\n",
        "    print(f\"Failed files: {failed_files}\")\n",
        "    print(f\"Missing files: {missing_files}\")\n",
        "    print(f\"Total files checked: {successful_files + failed_files + missing_files}\")\n",
        "\n",
        "    return np.array(data), np.array(labels)\n",
        "\n",
        "def load_object_data_and_labels(base_path, object_name):\n",
        "    object_data, object_labels = [], []\n",
        "    normalization_factors = {}\n",
        "\n",
        "    for person in range(1, PARTICIPANTS+1):\n",
        "        for folder in range(1, EXPERIMENTS+1):\n",
        "            file_path = os.path.join(base_path, str(person), FOLDER, str(folder), f'{object_name}_1.csv')\n",
        "            if os.path.exists(file_path):\n",
        "                try:\n",
        "                    df = read_csv_file(file_path)\n",
        "                    if IS_NORMALIZE:\n",
        "                        df, normalization_factors = normalize_features(df, \"imu_pos_y\", [\"imu_rot_x\"], user_id=str(person), normalization_factors=normalization_factors)\n",
        "                    if df is not None and not df.empty:\n",
        "                        features = extract_features(df)\n",
        "                        # features = extract_projection_features(df)\n",
        "                        object_data.append(features)\n",
        "                        object_labels.append(person - 1)\n",
        "                except Exception as e:\n",
        "                    continue\n",
        "    return object_data, object_labels\n",
        "\n",
        "def read_csv_file(file_path):\n",
        "    try:\n",
        "        with open(file_path, 'r') as file:\n",
        "            header_line = file.readline().strip()\n",
        "            columns = [col.strip() for col in header_line.split(',')]\n",
        "        data = pd.read_csv(file_path, skiprows=1, header=None, names=columns)\n",
        "        return data\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {file_path}: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def check_data_availability(base_path):\n",
        "    print(\"\\nChecking data availability:\")\n",
        "    for person in range(1, 5):\n",
        "        print(f\"\\nPerson {person}:\")\n",
        "        for folder in range(1, 11):\n",
        "            print(f\"\\nFolder {folder}:\")\n",
        "            for file_name in FILE_LIST:\n",
        "                file_path = os.path.join(base_path, str(person), FOLDER, str(folder), file_name)\n",
        "                status = \"âœ“\" if os.path.exists(file_path) else \"âœ—\"\n",
        "                print(f\"{status} {file_name}\")\n",
        "\n",
        "def normalize_features(df, reference_feature, other_features=[], user_id = None, normalization_factors={}):\n",
        "    if user_id not in normalization_factors:\n",
        "        # Compute initial value & scale factor from the reference feature\n",
        "        initial_value = df[reference_feature].iloc[0]\n",
        "        max_value = df[reference_feature].abs().max()\n",
        "\n",
        "        # Avoid division by zero\n",
        "        scale_factor = max_value if max_value != 0 else 1\n",
        "        normalization_factors[user_id] = (initial_value, scale_factor)\n",
        "    \n",
        "    # Apply stored normalization factors\n",
        "    initial_value, scale_factor = normalization_factors[user_id]\n",
        "    df[reference_feature] = (df[reference_feature] - initial_value) / scale_factor\n",
        "\n",
        "    # Apply the same transformation to other features\n",
        "    for feature in other_features:\n",
        "        df[feature] = (df[feature] - df[feature].iloc[0]) / scale_factor\n",
        "    \n",
        "    return df, normalization_factors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualization Functions\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_tsne_by_object():\n",
        "    plt.figure(figsize=(25, 15))\n",
        "    n_rows = 3\n",
        "    n_cols = 6\n",
        "    object_types = set()\n",
        "    for file in FILE_LIST:\n",
        "        if file.endswith('_1.csv'):\n",
        "            object_name = file.replace('_1.csv', '')\n",
        "            object_types.add(object_name)\n",
        "    plt.subplots_adjust(hspace=0.4, wspace=0.4)\n",
        "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
        "\n",
        "    for idx, object_name in enumerate(sorted(object_types)):\n",
        "        object_data, object_labels = load_object_data_and_labels(base_path, object_name)\n",
        "\n",
        "        if object_data:\n",
        "            ax = plt.subplot(n_rows, n_cols, idx + 1)\n",
        "            X_obj = np.array(object_data)\n",
        "            y_obj = np.array(object_labels)\n",
        "            # scaler = StandardScaler()\n",
        "            scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "            X_scaled = scaler.fit_transform(X_obj)\n",
        "            tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(X_scaled)-1))\n",
        "            X_tsne = tsne.fit_transform(X_scaled)\n",
        "            X_tsne = scaler.fit_transform(X_tsne)\n",
        "            for user in range(4):\n",
        "                mask = y_obj == user\n",
        "                plt.scatter(X_tsne[mask, 0], X_tsne[mask, 1],\n",
        "                            c=[colors[user]], label=f'User {user+1}',\n",
        "                            s=80, alpha=0.7, edgecolor='white', linewidth=0.5)\n",
        "\n",
        "            plt.title(object_name, fontsize=12, pad=10, fontweight='bold')\n",
        "            plt.xlabel('t-SNE 1', fontsize=10)\n",
        "            plt.ylabel('t-SNE 2', fontsize=10)\n",
        "            plt.xticks(fontsize=8)\n",
        "            plt.yticks(fontsize=8)\n",
        "            if idx == 0:\n",
        "                plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10, frameon=True)\n",
        "            else:\n",
        "                plt.legend([], [], frameon=False)\n",
        "            plt.grid(True, linestyle='--', alpha=0.3)\n",
        "            ax.spines['top'].set_visible(False)\n",
        "            ax.spines['right'].set_visible(False)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def visualize_pca_by_object():\n",
        "    plt.figure(figsize=(25, 15))\n",
        "    n_rows = 3\n",
        "    n_cols = 6\n",
        "    object_types = set()\n",
        "    for file in FILE_LIST:\n",
        "        if file.endswith('_1.csv'):\n",
        "            object_name = file.replace('_1.csv', '')\n",
        "            object_types.add(object_name)\n",
        "    plt.subplots_adjust(hspace=0.4, wspace=0.4)\n",
        "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
        "\n",
        "    for idx, object_name in enumerate(sorted(object_types)):\n",
        "        object_data, object_labels = load_object_data_and_labels(base_path, object_name)\n",
        "\n",
        "        if object_data:\n",
        "            ax = plt.subplot(n_rows, n_cols, idx + 1)\n",
        "            X_obj = np.array(object_data)\n",
        "            y_obj = np.array(object_labels)\n",
        "            # scaler = StandardScaler()\n",
        "            scaler = MinMaxScaler(feature_range=(0,1))\n",
        "            X_scaled = scaler.fit_transform(X_obj)\n",
        "            pca = PCA(n_components=2, random_state=42)\n",
        "            X_pca = pca.fit_transform(X_scaled)\n",
        "            X_pca = scaler.fit_transform(X_pca)\n",
        "            for user in range(4):\n",
        "                mask = y_obj == user\n",
        "                plt.scatter(X_pca[mask, 0], X_pca[mask, 1],\n",
        "                            c=[colors[user]], label=f'User {user+1}',\n",
        "                            s=80, alpha=0.7, edgecolor='white', linewidth=0.5)\n",
        "\n",
        "            plt.title(object_name, fontsize=12, pad=10, fontweight='bold')\n",
        "            plt.xlabel('PCA 1', fontsize=10)\n",
        "            plt.ylabel('PCA 2', fontsize=10)\n",
        "            plt.xticks(fontsize=8)\n",
        "            plt.yticks(fontsize=8)\n",
        "            if idx == 0:\n",
        "                plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10, frameon=True)\n",
        "            else:\n",
        "                plt.legend([], [], frameon=False)\n",
        "            plt.grid(True, linestyle='--', alpha=0.3)\n",
        "            ax.spines['top'].set_visible(False)\n",
        "            ax.spines['right'].set_visible(False)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def visualize_umap_by_object():\n",
        "    plt.figure(figsize=(25, 15))\n",
        "    n_rows, n_cols = 3, 6\n",
        "    object_types = {file.replace('_1.csv', '') for file in FILE_LIST if file.endswith('_1.csv')}\n",
        "\n",
        "    plt.subplots_adjust(hspace=0.4, wspace=0.4)\n",
        "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
        "\n",
        "    for idx, object_name in enumerate(sorted(object_types)):\n",
        "        object_data, object_labels = load_object_data_and_labels(base_path, object_name)\n",
        "\n",
        "        if object_data:\n",
        "            ax = plt.subplot(n_rows, n_cols, idx + 1)\n",
        "            X_obj = np.array(object_data)\n",
        "            y_obj = np.array(object_labels)\n",
        "\n",
        "            # ðŸ”¹ Ensure Data Consistency Before UMAP\n",
        "            if X_obj.shape[0] != len(y_obj):\n",
        "                print(f\"Skipping {object_name}: Feature and label size mismatch.\")\n",
        "                continue\n",
        "\n",
        "            # ðŸ”¹ Standardize Data\n",
        "            # scaler = StandardScaler()\n",
        "            scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "            X_scaled = scaler.fit_transform(X_obj)\n",
        "\n",
        "            # ðŸ”¹ Apply UMAP\n",
        "            umap_model = umap.UMAP(n_components=2, n_neighbors=5, min_dist=0.01, random_state=42, metric=\"correlation\")\n",
        "            X_umap = umap_model.fit_transform(X_scaled)\n",
        "            X_umap = scaler.fit_transform(X_umap)\n",
        "\n",
        "            # db = DBSCAN()\n",
        "            # X_dbscan_labels = db.fit_predict(X_umap)\n",
        "\n",
        "            # ðŸ”¹ Verify UMAP Output Matches Labels\n",
        "            if X_umap.shape[0] != len(y_obj):\n",
        "                print(f\"Skipping {object_name}: UMAP output and labels size mismatch.\")\n",
        "                continue  # Skip this object if still mismatched\n",
        "\n",
        "            # ðŸ”¹ Plot Results\n",
        "            for user in np.unique(y_obj):  # Dynamically get unique users\n",
        "                mask = (y_obj == user)\n",
        "\n",
        "                plt.scatter(\n",
        "                    X_umap[mask, 0], X_umap[mask, 1],\n",
        "                    c=[colors[user % len(colors)]], label=f'User {user+1}',\n",
        "                    s=80, alpha=0.7, edgecolor='white', linewidth=0.5\n",
        "                )\n",
        "\n",
        "            plt.title(object_name, fontsize=12, pad=10, fontweight='bold')\n",
        "            plt.xlabel('UMAP 1', fontsize=10)\n",
        "            plt.ylabel('UMAP 2', fontsize=10)\n",
        "            plt.xticks(fontsize=8)\n",
        "            plt.yticks(fontsize=8)\n",
        "\n",
        "            if idx == 0:\n",
        "                plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10, frameon=True)\n",
        "            else:\n",
        "                plt.legend([], [], frameon=False)\n",
        "\n",
        "            plt.grid(True, linestyle='--', alpha=0.3)\n",
        "            ax.spines['top'].set_visible(False)\n",
        "            ax.spines['right'].set_visible(False)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training and Classification\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "d0QmeoXlbq8o",
        "outputId": "383acddb-8986-4651-ca13-5362b577875a"
      },
      "outputs": [],
      "source": [
        "def train_and_evaluate_model(X, y):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=42)\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    rf_classifier.fit(X_train_scaled, y_train)\n",
        "    y_pred = rf_classifier.predict(X_test_scaled)\n",
        "\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.show()\n",
        "\n",
        "    return rf_classifier, scaler\n",
        "\n",
        "def print_classification_results(X, y, method=\"rfc\", object_name=None):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y,\n",
        "        test_size=TEST_SIZE,\n",
        "        random_state=42,\n",
        "        stratify=y\n",
        "    )\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    if method == \"gbc\":\n",
        "        print(\"Using Gradient Boosting Classifier\")\n",
        "        rf_classifier = GradientBoostingClassifier(\n",
        "            n_estimators=100, \n",
        "            random_state=42, \n",
        "            max_depth=10, \n",
        "            min_samples_split=5, \n",
        "            min_samples_leaf=2, \n",
        "            max_features=\"sqrt\",\n",
        "            \n",
        "        )\n",
        "    else:\n",
        "        print(\"Using Random Forest Classifier\")\n",
        "        rf_classifier = RandomForestClassifier(\n",
        "            n_estimators=100, \n",
        "            random_state=42,\n",
        "            max_depth=10,\n",
        "            min_samples_split=5,\n",
        "            min_samples_leaf=2,\n",
        "            max_features=\"sqrt\",\n",
        "            class_weight=\"balanced\",\n",
        "            n_jobs=-1\n",
        "        )\n",
        "\n",
        "    rf_classifier.fit(X_train_scaled, y_train)\n",
        "    y_pred = rf_classifier.predict(X_test_scaled)\n",
        "    feature_importance = calculate_feature_importance(rf_classifier)\n",
        "\n",
        "    print(\"\\nData Distribution:\")\n",
        "    unique, counts = np.unique(y, return_counts=True)\n",
        "    print(\"Total samples per user:\", dict(zip([f\"User {i+1}\" for i in unique], counts)))\n",
        "\n",
        "    unique_test, counts_test = np.unique(y_test, return_counts=True)\n",
        "    print(\"Test set samples per user:\", dict(zip([f\"User {i+1}\" for i in unique_test], counts_test)))\n",
        "\n",
        "    title = f\"\\nClassification Results for {object_name}\" if object_name else \"\\nOverall Classification Results\"\n",
        "    print(title)\n",
        "    print(\"-\" * len(title))\n",
        "\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    print(\"True\\\\Pred |  User1  User2  User3  User4\")\n",
        "    print(\"-\" * 40)\n",
        "    for i, row in enumerate(cm):\n",
        "        print(f\"User {i+1}    | {str(row).center(20)}\")\n",
        "\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    print(\"\\nTop 10 Features:\")\n",
        "    print(feature_importance.sort_values(ascending=False).head(10))\n",
        "    print(\"\\n\")\n",
        "    \n",
        "    return rf_classifier\n",
        "\n",
        "def print_perobject_classification_results():\n",
        "    object_types = set()\n",
        "    for file in FILE_LIST:\n",
        "        if file.endswith('_1.csv'):\n",
        "            object_name = file.replace('_1.csv', '')\n",
        "            object_types.add(object_name)\n",
        "\n",
        "    print(\"\\nPer-Object Classification Results:\")\n",
        "    print(\"==================================\")\n",
        "    for object_name in sorted(object_types):\n",
        "        object_data, object_labels = load_object_data_and_labels(base_path, object_name)\n",
        "\n",
        "        if object_data:\n",
        "            X_obj = np.array(object_data)\n",
        "            y_obj = np.array(object_labels)\n",
        "            print_classification_results(X_obj, y_obj, object_name=object_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Main\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Loading data from: {base_path}\")\n",
        "X, y = load_data(base_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def main():\n",
        "    print_classification_results(X, y)\n",
        "    print_classification_results(X, y, method=\"gbc\")\n",
        "\n",
        "    # Per-object classification results\n",
        "    # print_perobject_classification_results()\n",
        "    \n",
        "    # Visualize clusters using the three methods\n",
        "    \n",
        "    # print(\"\\nVisualizing clusters using t-SNE:\")\n",
        "    # visualize_tsne_by_object(X, y, FILE_LIST, base_path)\n",
        "\n",
        "    # print(\"\\nVisualizing clusters using PCA:\")\n",
        "    # visualize_pca_by_object(X, y, FILE_LIST, base_path)\n",
        "\n",
        "    # print(\"\\nVisualizing clusters using UMAP:\")\n",
        "    # visualize_umap_by_object(FILE_LIST, base_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "base_path = os.path.join(os.getcwd(), ROOT)\n",
        "visualize_umap_by_object(FILE_LIST, base_path)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
